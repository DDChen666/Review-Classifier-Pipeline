#!/usr/bin/env python3
"""
æ•¸æ“šæ¸…æ´—èˆ‡å¾…æ¨™è¨»é›†ç”Ÿæˆå·¥å…·

åŠŸèƒ½ï¼š
- è®€å–åˆä½µè©•è«–CSVæ–‡ä»¶
- æ•¸æ“šæ¸…æ´—ï¼šè½‰æ›å…¨å½¢ç¬¦è™Ÿç‚ºåŠå½¢ã€ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿã€ç§»é™¤çŸ­æ–‡æœ¬
- è¼¸å‡ºå–®ä¸€çš„JSONæ ¼å¼æ–‡ä»¶ï¼Œç”¨æ–¼å¾ŒçºŒçš„æ•¸æ“šæ¨™è¨»

JSONæ ¼å¼ï¼š
[
    {"text": "è©•è«–å…§å®¹", "primary": "", "secondary": ""},
    ...
]
"""

import os
import sys
import pandas as pd
import json
import re
from datetime import datetime
import random
import glob
import unicodedata


def find_merged_reviews_file(output_dir):
    """
    æŸ¥æ‰¾æœ€æ–°çš„merged_reviews CSVæ–‡ä»¶

    Args:
        output_dir (str): è¼¸å‡ºç›®éŒ„è·¯å¾‘

    Returns:
        str: CSVæ–‡ä»¶è·¯å¾‘ï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å›None
    """
    try:
        # æŸ¥æ‰¾æ‰€æœ‰merged_reviews_*.csvæ–‡ä»¶
        pattern = os.path.join(output_dir, "merged_reviews_*.csv")
        files = glob.glob(pattern)

        if not files:
            print("âœ— æ‰¾ä¸åˆ°ä»»ä½•merged_reviewsæ–‡ä»¶")
            return None

        # å¦‚æœæœ‰å¤šå€‹æ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
        latest_file = max(files, key=os.path.getmtime)
        print(f"âœ“ æ‰¾åˆ°æœ€æ–°çš„merged_reviewsæ–‡ä»¶ï¼š{os.path.basename(latest_file)}")
        return latest_file

    except Exception as e:
        print(f"âœ— æŸ¥æ‰¾æ–‡ä»¶å¤±æ•—ï¼š{e}")
        return None


def load_merged_reviews(csv_path):
    """
    åŠ è¼‰åˆä½µè©•è«–CSVæ–‡ä»¶

    Args:
        csv_path (str): CSVæ–‡ä»¶è·¯å¾‘

    Returns:
        pd.DataFrame: åŠ è¼‰çš„DataFrame
    """
    try:
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        print(f"âœ“ æˆåŠŸåŠ è¼‰ {len(df)} æ¢è©•è«–æ•¸æ“š")
        return df
    except Exception as e:
        print(f"âœ— åŠ è¼‰CSVæ–‡ä»¶å¤±æ•—ï¼š{e}")
        return pd.DataFrame()


def convert_fullwidth_to_halfwidth(text):
    """
    å°‡å…¨å½¢å­—ç¬¦è½‰æ›ç‚ºåŠå½¢å­—ç¬¦

    Args:
        text (str): è¼¸å…¥æ–‡æœ¬

    Returns:
        str: è½‰æ›å¾Œçš„æ–‡æœ¬
    """
    if not isinstance(text, str):
        return ""

    fullwidth_chars = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼ï¼‚ï¼ƒï¼„ï¼…ï¼†ï¼‡ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼Ÿï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ã€€"
    halfwidth_chars = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ "
    trans_table = str.maketrans(fullwidth_chars, halfwidth_chars)
    return text.translate(trans_table)


def remove_emojis(text):
    """
    ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿå’Œç‰¹æ®Šç¬¦è™Ÿ - æ”¹è¿›ç‰ˆæœ¬
    ä½¿ç”¨æ›´å…¨é¢çš„UnicodeèŒƒå›´æ¥åŒ¹é…å„ç§è¡¨æƒ…ç¬¦å·

    Args:
        text (str): è¼¸å…¥æ–‡æœ¬

    Returns:
        str: ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿå¾Œçš„æ–‡æœ¬
    """
    if not isinstance(text, str):
        return ""
    
    # æ›´å…¨é¢çš„è¡¨æƒ…ç¬¦å·UnicodeèŒƒå›´
    emoji_patterns = [
        # åŸºç¡€è¡¨æƒ…ç¬¦å·
        r'[\U0001F600-\U0001F64F]',  # emoticons
        r'[\U0001F300-\U0001F5FF]',  # symbols & pictographs
        r'[\U0001F680-\U0001F6FF]',  # transport & map symbols
        r'[\U0001F1E0-\U0001F1FF]',  # flags (iOS)
        
        # æ‰©å±•è¡¨æƒ…ç¬¦å·èŒƒå›´
        r'[\U0001F900-\U0001F9FF]',  # supplemental symbols and pictographs
        r'[\U0001FA00-\U0001FA6F]',  # chess symbols
        r'[\U0001FA70-\U0001FAFF]',  # symbols and pictographs extended-A
        r'[\U00002600-\U000026FF]',  # miscellaneous symbols
        r'[\U00002700-\U000027BF]',  # dingbats
        
        # å…¶ä»–ç›¸å…³ç¬¦å·
        r'[\U0001F004\U0001F0CF]',   # mahjong and playing cards
        r'[\U0001F170-\U0001F251]',  # enclosed characters
        r'[\U0000FE00-\U0000FE0F]',  # variation selectors
        r'[\U00002000-\U0000206F]',  # general punctuation (åŒ…æ‹¬é›¶å®½å­—ç¬¦)
        r'[\U0000200D]',             # zero width joiner
        r'[\U0000FE0F]',             # variation selector-16
        
        # ç‰¹æ®Šå­—ç¬¦
        r'[â„¢Â©Â®]',                    # trademark, copyright, registered
        r'[\u2122\u00A9\u00AE]',     # åŒä¸Šï¼Œä¸åŒç¼–ç 
        
        # å…¶ä»–å¯èƒ½çš„è¡¨æƒ…ç¬¦å·èŒƒå›´
        r'[\U0001F780-\U0001F7FF]',  # geometric shapes extended
        r'[\U0001F800-\U0001F8FF]',  # supplemental arrows-C
    ]
    
    result = text
    for pattern in emoji_patterns:
        result = re.sub(pattern, '', result, flags=re.UNICODE)
    
    # é¢å¤–å¤„ç†ï¼šç§»é™¤è¿ç»­çš„ä¿®é¥°ç¬¦å’Œç»„åˆå­—ç¬¦
    # è¿™äº›å­—ç¬¦é€šå¸¸ä¸è¡¨æƒ…ç¬¦å·ä¸€èµ·å‡ºç°
    modifiers_pattern = r'[\U0001F3FB-\U0001F3FF]+'  # skin tone modifiers
    result = re.sub(modifiers_pattern, '', result, flags=re.UNICODE)
    
    # ä½¿ç”¨unicodedataæ¥è¯†åˆ«å¹¶ç§»é™¤å…¶ä»–å¯èƒ½çš„ç¬¦å·å­—ç¬¦
    # ä½†è¦ä¿ç•™æ ‡ç‚¹ç¬¦å·å’Œå…¶ä»–æœ‰ç”¨å­—ç¬¦
    cleaned_chars = []
    for char in result:
        # è·å–å­—ç¬¦çš„Unicodeç±»åˆ«
        category = unicodedata.category(char)
        # ä¿ç•™å­—æ¯ã€æ•°å­—ã€æ ‡ç‚¹ã€ç©ºæ ¼ã€ä¸­æ–‡ç­‰æœ‰ç”¨å­—ç¬¦
        if category[0] in ['L', 'N', 'P', 'Z', 'M'] or unicodedata.name(char, '').startswith('CJK'):
            # ä½†æ’é™¤ä¸€äº›ç‰¹æ®Šç¬¦å·ç±»åˆ«
            if not (category == 'So' and ord(char) > 0x1F000):  # æ’é™¤é«˜ä½ç¬¦å·å­—ç¬¦
                cleaned_chars.append(char)
    
    result = ''.join(cleaned_chars)
    return result


def count_chinese_characters(text):
    """
    è¨ˆç®—ä¸­æ–‡å­—ç¬¦æ•¸é‡

    Args:
        text (str): è¼¸å…¥æ–‡æœ¬

    Returns:
        int: ä¸­æ–‡å­—ç¬¦æ•¸é‡
    """
    if not isinstance(text, str):
        return 0
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
    return len(chinese_pattern.findall(text))


def clean_text(text):
    """
    æ¸…ç†æ–‡æœ¬ï¼šè½‰æ›å…¨å½¢ç‚ºåŠå½¢ã€ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿ

    Args:
        text (str): è¼¸å…¥æ–‡æœ¬

    Returns:
        str: æ¸…ç†å¾Œçš„æ–‡æœ¬
    """
    if not isinstance(text, str):
        return ""
    text = convert_fullwidth_to_halfwidth(text)
    text = remove_emojis(text)
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦ï¼Œä½†ä¿ç•™å•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def filter_short_texts(data_list, min_chinese_chars=2):
    """
    éæ¿¾æ‰ä¸­æ–‡å­—ç¬¦æ•¸é‡ä¸è¶³çš„æ–‡æœ¬

    Args:
        data_list (list): æ•¸æ“šåˆ—è¡¨
        min_chinese_chars (int): æœ€å°‘ä¸­æ–‡å­—ç¬¦æ•¸

    Returns:
        list: éæ¿¾å¾Œçš„æ•¸æ“šåˆ—è¡¨
    """
    filtered_data = []
    removed_count = 0

    for item in data_list:
        text = item.get('text', '')
        chinese_count = count_chinese_characters(text)
        if chinese_count > min_chinese_chars:
            filtered_data.append(item)
        else:
            removed_count += 1

    print(f"âœ“ ç§»é™¤äº† {removed_count} æ¢çŸ­æ–‡æœ¬ï¼ˆä¸­æ–‡å­—ç¬¦ â‰¤ {min_chinese_chars}ï¼‰")
    return filtered_data


def save_json_file(data, file_path):
    """
    ä¿å­˜æ•¸æ“šç‚ºJSONæ–‡ä»¶

    Args:
        data (list): è¦ä¿å­˜çš„æ•¸æ“š
        file_path (str): æ–‡ä»¶è·¯å¾‘
    """
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ å·²ä¿å­˜ {len(data)} æ¢æ•¸æ“šåˆ° {file_path}")
    except Exception as e:
        print(f"âœ— ä¿å­˜JSONæ–‡ä»¶å¤±æ•—ï¼š{e}")


def test_emoji_removal():
    """
    æµ‹è¯•è¡¨æƒ…ç¬¦å·ç§»é™¤åŠŸèƒ½
    """
    test_cases = [
        "è¿™æ˜¯æµ‹è¯•æ–‡æœ¬ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜‡",
        "å¥½æ£’ğŸ‘ğŸ‘ğŸ‘ŒğŸ‘ŠâœŠğŸ¤›ğŸ¤œğŸ¤âœŒï¸ğŸ¤ŸğŸ¤˜ğŸ‘",
        "å¿ƒæƒ…ğŸ’•ğŸ’–ğŸ’—ğŸ’˜ğŸ’ğŸ’ŸğŸ’œğŸ–¤ğŸ’›ğŸ’šğŸ’™ğŸ’”â¤ï¸",
        "å¤©æ°”â˜€ï¸â›…â›ˆï¸ğŸŒ§ï¸âŒ¨ï¸ğŸ“±ğŸ’»ğŸ–¥ï¸ğŸ–¨ï¸âŒšğŸ“·ğŸ“¹ğŸ¥ğŸ“â˜ï¸ğŸ“ ",
        "é£Ÿç‰©ğŸğŸŠğŸ‹ğŸŒğŸ‰ğŸ‡ğŸ“ğŸ«ğŸˆğŸ’ğŸ‘ğŸ¥­ğŸğŸ¥¥ğŸ¥",
        "æ™®é€šæ ‡ç‚¹ç¬¦å·ï¼šï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹",
        "è¿™é‡Œæœ‰â„¢ç‰ˆæƒÂ©ç¬¦å·å’ŒÂ®æ³¨å†Œå•†æ ‡",
        "æ··åˆå†…å®¹ğŸ˜€è¿™æ˜¯ä¸­æ–‡ğŸ‰EnglishğŸ˜Š123æ•°å­—ï¼@#$%ç¬¦å·"
    ]
    
    print("è¡¨æƒ…ç¬¦å·æ¸…é™¤æµ‹è¯•ï¼š")
    print("-" * 50)
    for i, test_text in enumerate(test_cases, 1):
        cleaned = remove_emojis(test_text)
        print(f"{i}. åŸå§‹: {test_text}")
        print(f"   æ¸…ç†: {cleaned}")
        print()


def main():
    """ä¸»ç¨‹å¼"""

    print("=" * 60)
    print("æ•¸æ“šæ¸…æ´—èˆ‡å¾…æ¨™è¨»é›†ç”Ÿæˆå·¥å…·")
    print("=" * 60)

    # å¯é€‰ï¼šè¿è¡Œè¡¨æƒ…ç¬¦å·æ¸…é™¤æµ‹è¯•
    # test_emoji_removal()
    # return

    # è¨­å®šè·¯å¾‘
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "output")

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    print(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
    print(f"é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # 1. æŸ¥æ‰¾ä¸¦åŠ è¼‰åˆä½µå¾Œçš„CSVæ–‡ä»¶
    input_file = find_merged_reviews_file(output_dir)
    if not input_file:
        return
    df = load_merged_reviews(input_file)
    if df.empty:
        print("âœ— æ²’æœ‰æœ‰æ•ˆçš„è©•è«–æ•¸æ“š")
        return
    print()

    # 2. æ•¸æ“šæ¸…æ´—
    print("2. æ•¸æ“šæ¸…æ´—...")
    cleaned_data = []
    for _, row in df.iterrows():
        original_text = str(row.get('content', ''))
        if not original_text.strip():
            continue
        cleaned_text = clean_text(original_text)
        data_item = {
            "text": cleaned_text,
            "primary": "",
            "secondary": ""
        }
        cleaned_data.append(data_item)
    print(f"âœ“ åŸå§‹æ•¸æ“š: {len(df)} æ¢ï¼Œæ¸…æ´—å¾Œå‰©é¤˜: {len(cleaned_data)} æ¢")
    print()

    # 3. éæ¿¾çŸ­æ–‡æœ¬
    print("3. éæ¿¾çŸ­æ–‡æœ¬...")
    filtered_data = filter_short_texts(cleaned_data, min_chinese_chars=2)
    print(f"âœ“ éæ¿¾å¾Œæ•¸æ“š: {len(filtered_data)} æ¢")
    print()

    if not filtered_data:
        print("âœ— æ²’æœ‰è¶³å¤ çš„æ•¸æ“šå¯ä¾›æ¨™è¨»ï¼Œç¨‹åºçµ‚æ­¢ã€‚")
        return

    # 4. ä¿å­˜ç‚ºå¾…æ¨™è¨»æ–‡ä»¶
    print("4. ä¿å­˜ç‚ºå¾…æ¨™è¨»çš„JSONæ–‡ä»¶...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f"unlabeled_reviews_{timestamp}.json"
    output_path = os.path.join(output_dir, output_filename)
    save_json_file(filtered_data, output_path)
    print()

    # é¡¯ç¤ºæ¨£ä¾‹
    print("5. æ•¸æ“šæ¨£ä¾‹:")
    for i, item in enumerate(filtered_data[:3]):
        print(f"  {i+1}. {item}")
    print()

    # ç¸½çµ
    print("=" * 60)
    print("è™•ç†å®Œæˆç¸½çµ:")
    print(f"åŸå§‹è©•è«–æ•¸é‡: {len(df)}")
    print(f"æ¸…æ´—å¾Œæ•¸é‡: {len(cleaned_data)}")
    print(f"æœ€çµ‚æœ‰æ•ˆæ•¸æ“š: {len(filtered_data)}")
    print(f"è¼¸å‡ºæ–‡ä»¶: {output_filename}")
    print(f"å®Œæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

if __name__ == "__main__":
    main()